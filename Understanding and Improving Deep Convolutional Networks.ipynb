{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cognitive-staff",
   "metadata": {},
   "source": [
    "# Tracking learning\n",
    "\n",
    "While training the model, the weights in the model would change. As the weights change, the network becomes more attuned to the features of the images that discrimnate between classes, and it means the loss function becomes smaller and smaller\n",
    "\n",
    "因此可以透過檢視每次loss function的值來評估模型是否足夠好\n",
    "\n",
    "The example graph shows the categorical cross-entropy loss in the network (comparing between training dataset and validation_split dataset)\n",
    "![](Image/Image11.jpg)\n",
    "\n",
    "可以看出overfitting的現象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-influence",
   "metadata": {},
   "source": [
    "## Tracking the loss function in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-adobe",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dried-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of one-hot encoding\n",
    "import numpy as np\n",
    "\n",
    "def dummies(n, labels):\n",
    "    # The number of image categories\n",
    "    n_categories = n\n",
    "\n",
    "    # The unique values of categories in the data\n",
    "    categories = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "    # Initialize ohe_labels as all zeros\n",
    "    ohe_labels = np.zeros((len(labels), n_categories))\n",
    "\n",
    "    # Loop over the labels\n",
    "    for ii in range(len(labels)):\n",
    "        # Find the location of this label in the categories variable\n",
    "        jj = np.where(labels[ii] == categories)\n",
    "        # Set the corresponding zero to one\n",
    "        ohe_labels[ii,jj] = 1\n",
    "\n",
    "    return ohe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dramatic-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"MNIST dataset\\\\mnist.csv\", header=None)\n",
    "X = data.drop(0, axis = 1).to_numpy()\n",
    "y_raw = data[0].to_numpy()\n",
    "y = dummies(10, y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mineral-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "communist-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "israeli-loading",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 480 samples\n",
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 0s 281us/step - loss: 20.8434 - accuracy: 0.5616 - val_loss: 8.2814 - val_accuracy: 0.7417\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 0s 56us/step - loss: 3.4228 - accuracy: 0.8393 - val_loss: 5.5508 - val_accuracy: 0.7896\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 1.4160 - accuracy: 0.9098 - val_loss: 5.2529 - val_accuracy: 0.8021\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.4379 - accuracy: 0.9545 - val_loss: 5.2970 - val_accuracy: 0.8167\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.4308 - accuracy: 0.9643 - val_loss: 5.1064 - val_accuracy: 0.8083\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.4136 - accuracy: 0.9571 - val_loss: 4.8979 - val_accuracy: 0.8333\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - 0s 70us/step - loss: 0.2729 - accuracy: 0.9723 - val_loss: 4.0916 - val_accuracy: 0.8375\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - 0s 62us/step - loss: 0.1162 - accuracy: 0.9821 - val_loss: 4.5702 - val_accuracy: 0.8375\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - 0s 90us/step - loss: 0.1106 - accuracy: 0.9830 - val_loss: 4.1979 - val_accuracy: 0.8562\n",
      "Epoch 10/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.1697 - accuracy: 0.9857 - val_loss: 4.3235 - val_accuracy: 0.8687\n"
     ]
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(160, activation = 'relu', input_shape = (784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "\n",
    "# Add the third hidden layer\n",
    "#model.add(Dense(50, activation = 'relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Set up Early Stopping Monitor\n",
    "early_stopping_monitor = EarlyStopping(patience = 3)\n",
    "\n",
    "# Fit the model\n",
    "training = model.fit(X_train, y_train, validation_split = 0.3, epochs = 20, callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-alert",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "backed-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_digits = model.predict_classes(X_test)\n",
    "y_pred = dummies(10, classified_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fifth-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343.0\n",
      "The prediction accuracy of the handwritten-digit recognition model is 85.536%\n"
     ]
    }
   ],
   "source": [
    "number_correct = (y_pred*y_test).sum()\n",
    "print(number_correct)\n",
    "accuracy = number_correct/len(y_pred)\n",
    "print(\"The prediction accuracy of the handwritten-digit recognition model is \" + str(round(accuracy*100, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-cartridge",
   "metadata": {},
   "source": [
    "Plot the loss function of training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demonstrated-population",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training.history[\"loss\"])\n",
    "plt.plot(training.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-transcription",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worthy-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('Model_saved/Handwritten_digit_classification.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-rapid",
   "metadata": {},
   "source": [
    "## Use the best parameters before the network starts overfitting \n",
    "\n",
    "使用一種Callback function (callback function會在每次epoch跑完後就執行一次，目的去檢查是否訓練可以停下)\n",
    "\n",
    "checkpoint會去紀錄模型訓練過程中最低的loss值，只要發現loss值降低就會更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('weights.hdf5', monitor = 'val_loss', save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-series",
   "metadata": {},
   "source": [
    "Fit the model again using the new callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "grand-vancouver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 480 samples\n",
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 0s 84us/step - loss: 0.2480 - accuracy: 0.9857 - val_loss: 5.3355 - val_accuracy: 0.8333\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 0s 62us/step - loss: 0.3575 - accuracy: 0.9714 - val_loss: 5.3828 - val_accuracy: 0.8146\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 0s 62us/step - loss: 0.2502 - accuracy: 0.9804 - val_loss: 4.3039 - val_accuracy: 0.8583\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 0s 62us/step - loss: 0.2771 - accuracy: 0.9786 - val_loss: 5.7710 - val_accuracy: 0.8417\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 0s 62us/step - loss: 0.4747 - accuracy: 0.9705 - val_loss: 4.5991 - val_accuracy: 0.8417\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.3286 - accuracy: 0.9795 - val_loss: 5.4381 - val_accuracy: 0.8479\n"
     ]
    }
   ],
   "source": [
    "# Fit the model again\n",
    "training = model.fit(X_train, y_train, validation_split = 0.3, epochs = 20, callbacks = [early_stopping_monitor, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-particle",
   "metadata": {},
   "source": [
    "### Load the best weights with the lowest loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "portable-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-sterling",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cognitive-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351.0\n",
      "The prediction accuracy of the handwritten-digit recognition model is 87.531%\n"
     ]
    }
   ],
   "source": [
    "classified_digits = model.predict_classes(X_test)\n",
    "y_pred = dummies(10, classified_digits)\n",
    "number_correct = (y_pred*y_test).sum()\n",
    "print(number_correct)\n",
    "accuracy = number_correct/len(y_pred)\n",
    "print(\"The prediction accuracy of the handwritten-digit recognition model is \" + str(round(accuracy*100, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-brook",
   "metadata": {},
   "source": [
    "The accuracy is improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-cinema",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-saturday",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "\n",
    "Two major strategies of regularisation:\n",
    "1. Dropout\n",
    "2. Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-duplicate",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "In each step of learning, we randomly choose a subset of the units in a layer and ignore it\n",
    "\n",
    "Concept Visualisation:\n",
    "![](Image/Image12.jpg)\n",
    "\n",
    "This technique allows us to train different networks on different parts of the data. Therefore, if part of the network is too sensitive to some noise in the data, other parts will compensate for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-armstrong",
   "metadata": {},
   "source": [
    "## Dropout in Keras\n",
    "\n",
    "Dropout is implemented as a layer. We add a layer after the layer we want units ignored.\n",
    "\n",
    "In each dropout layer, we have to specify the proportion of the units in the layer to ignore in each learning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liberal-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import dropout layers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loaded-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 480 samples\n",
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 1s 1ms/step - loss: 33.8432 - accuracy: 0.4643 - val_loss: 8.9746 - val_accuracy: 0.7167\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 0s 73us/step - loss: 8.8530 - accuracy: 0.7080 - val_loss: 6.4547 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 0s 75us/step - loss: 4.3794 - accuracy: 0.7929 - val_loss: 3.9738 - val_accuracy: 0.7896\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 0s 73us/step - loss: 3.0876 - accuracy: 0.8098 - val_loss: 3.5348 - val_accuracy: 0.8021\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 0s 81us/step - loss: 2.3597 - accuracy: 0.8259 - val_loss: 3.3188 - val_accuracy: 0.8354\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 0s 80us/step - loss: 1.7177 - accuracy: 0.8705 - val_loss: 2.9807 - val_accuracy: 0.8333\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - 0s 81us/step - loss: 1.4173 - accuracy: 0.8821 - val_loss: 3.0905 - val_accuracy: 0.8313\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 1.1273 - accuracy: 0.8920 - val_loss: 3.0893 - val_accuracy: 0.8271\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - 0s 75us/step - loss: 0.7907 - accuracy: 0.9179 - val_loss: 3.0236 - val_accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(160, activation = 'relu', input_shape = (784,)))\n",
    "\n",
    "# ==== Dropout ====\n",
    "# Dropout 0.25 units in each step\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# The rest of the model remains the same\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Set up Early Stopping Monitor\n",
    "early_stopping_monitor = EarlyStopping(patience = 3)\n",
    "\n",
    "# Fit the model\n",
    "training = model.fit(X_train, y_train, validation_split = 0.3, epochs = 20, callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "after-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.0\n",
      "The prediction accuracy of the handwritten-digit recognition model is 83.292%\n"
     ]
    }
   ],
   "source": [
    "classified_digits = model.predict_classes(X_test)\n",
    "y_pred = dummies(10, classified_digits)\n",
    "number_correct = (y_pred*y_test).sum()\n",
    "print(number_correct)\n",
    "accuracy = number_correct/len(y_pred)\n",
    "print(\"The prediction accuracy of the handwritten-digit recognition model is \" + str(round(accuracy*100, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-cancellation",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-cyprus",
   "metadata": {},
   "source": [
    "## Batch Normalisation\n",
    "\n",
    "Standardise the output of a specified layer to have 0 mean and standard deviation of 1\n",
    "\n",
    "Different batches of input might produce wildly different distributions of outputs in any given layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-shift",
   "metadata": {},
   "source": [
    "## Batch Normalisation in Keras\n",
    "\n",
    "This technique is also implemented as layers in Keras. <br/>\n",
    "We add a layer after the layer we want to scale the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecological-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dropout layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fatty-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 480 samples\n",
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 1s 562us/step - loss: 1.1503 - accuracy: 0.6464 - val_loss: 0.7172 - val_accuracy: 0.7979\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 0s 82us/step - loss: 0.3273 - accuracy: 0.9250 - val_loss: 0.5120 - val_accuracy: 0.8479\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 0s 84us/step - loss: 0.1798 - accuracy: 0.9696 - val_loss: 0.4026 - val_accuracy: 0.8854\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 0s 83us/step - loss: 0.1097 - accuracy: 0.9821 - val_loss: 0.3892 - val_accuracy: 0.8771\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 0s 78us/step - loss: 0.0697 - accuracy: 0.9902 - val_loss: 0.3470 - val_accuracy: 0.8792\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.0511 - accuracy: 0.9946 - val_loss: 0.3390 - val_accuracy: 0.8896\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - 0s 78us/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.3337 - val_accuracy: 0.8833\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - 0s 79us/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 0.8938\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - 0s 78us/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.3267 - val_accuracy: 0.9000\n",
      "Epoch 10/20\n",
      "1120/1120 [==============================] - 0s 79us/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3404 - val_accuracy: 0.8979\n",
      "Epoch 11/20\n",
      "1120/1120 [==============================] - 0s 76us/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3347 - val_accuracy: 0.8979\n"
     ]
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(160, activation = 'relu', input_shape = (784,)))\n",
    "\n",
    "# ==== Batch Normalization ====\n",
    "# Normalise the output of the previous layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# The rest of the model remains the same\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Set up Early Stopping Monitor\n",
    "early_stopping_monitor = EarlyStopping(patience = 3)\n",
    "\n",
    "# Fit the model\n",
    "training = model.fit(X_train, y_train, validation_split = 0.3, epochs = 20, callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decreased-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357.0\n",
      "The prediction accuracy of the handwritten-digit recognition model is 89.027%\n"
     ]
    }
   ],
   "source": [
    "classified_digits = model.predict_classes(X_test)\n",
    "y_pred = dummies(10, classified_digits)\n",
    "number_correct = (y_pred*y_test).sum()\n",
    "print(number_correct)\n",
    "accuracy = number_correct/len(y_pred)\n",
    "print(\"The prediction accuracy of the handwritten-digit recognition model is \" + str(round(accuracy*100, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-shower",
   "metadata": {},
   "source": [
    "## The disharmony between Batch Normalisation and Dropout\n",
    "\n",
    "Sometimes, dropout and batch normalisation do not work well together.\n",
    "\n",
    "Dropout tends to **slow down** the learning process (more careful), while batch normalisation tends to make training processes **faster**. Therefore, when we use both techniques simultaneously, they may produce a worse result than the model without regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-latest",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-party",
   "metadata": {},
   "source": [
    "# Interpretation of models\n",
    "\n",
    "Visualisating what different parts of the network are doing: Select a particular part of a convolutional network and analyse its behaviour.\n",
    "\n",
    "Once the model is compiled, it will store its layers in an attribute called \"layers\" in the model object. This attribute is a list of layer objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-albania",
   "metadata": {},
   "source": [
    "## Look deeper in each layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "turned-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import convolution layers in 2-Dimensional image\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Use a Flatten layer to be like a bridge between a convolution layer and a dense layer\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# Other modules are the same\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential    # 仍然使用Sequential model\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aggregate-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# first cnn layer\n",
    "model.add(Conv2D(10, kernel_size = 2, activation = 'relu', input_shape = (28, 28, 1), padding = 'same'))\n",
    "\n",
    "# Batch normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# second cnn layer\n",
    "model.add(Conv2D(10, kernel_size = 2, activation = 'relu'))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Compile th model\n",
    "model.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "criminal-copying",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x23d40dd2748>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x23d40dd26c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x23d3f9fd308>,\n",
       " <keras.layers.core.Flatten at 0x23d40ddd408>,\n",
       " <keras.layers.core.Dense at 0x23d40e01048>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "extended-wedding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x23d40dd2748>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FirstConvlayer = model.layers[0]\n",
    "FirstConvlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-defense",
   "metadata": {},
   "source": [
    "Get weights of a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mediterranean-covering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[[[ 0.27411816 -0.30638087  0.2665628   0.0738984   0.14653984\n",
      "     0.3596767   0.19624433  0.27652887 -0.05711189  0.10905865]]\n",
      "\n",
      "  [[-0.16108902 -0.1684348   0.03092727 -0.28043404 -0.0913468\n",
      "     0.28214476  0.25788632 -0.3375675  -0.00346699  0.00801867]]]\n",
      "\n",
      "\n",
      " [[[ 0.32791778  0.06103742  0.10251573  0.29084352 -0.05236503\n",
      "    -0.02372387 -0.07497877  0.1488047   0.27222177 -0.04544511]]\n",
      "\n",
      "  [[-0.16969731  0.07483253 -0.3614863   0.12685612 -0.3161542\n",
      "    -0.15200064  0.22860458 -0.09235242 -0.08514249  0.2522103 ]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 1, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_weights function will return an object contains two components.\n",
    "weights_FirstConvlayer = FirstConvlayer.get_weights()\n",
    "print(len(weights_FirstConvlayer))\n",
    "\n",
    "# The first component: an array holds the values of the weights for the convolutional kernels for this layer\n",
    "print(weights_FirstConvlayer[0])\n",
    "weights_FirstConvlayer[0].shape\n",
    "# The first two dimensions denote the kernel size\n",
    "# The third dimension denotes the number of channels in the kernels. Here the value is 1 because the model is looking at black and white data\n",
    "# The last dimension denotes the number of kernels in this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-brick",
   "metadata": {},
   "source": [
    "Pull out the first kernel in this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "composite-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.27411816, -0.16108902],\n",
       "       [ 0.32791778, -0.16969731]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = weights_FirstConvlayer[0]\n",
    "kernel1_L1 = layer1[:,:,0,0]\n",
    "print(kernel1_L1.shape)\n",
    "kernel1_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "thick-louisville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23d3fcbaa88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(kernel1_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-developer",
   "metadata": {},
   "source": [
    "1. 透過kernel的視覺化來詮釋此kernel的功用是什麼 (負責identify點、垂直線、水平線或更複雜的物件)\n",
    "2. 可以將這個kernel跑過一張image然後visulise結果，看看結果跟原始圖片的差異\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "![](Image/Image13.jpg)\n",
    "\n",
    "![](Image/Image14.jpg)\n",
    "\n",
    "It seems that this kernel is effective in identifying the external edges of this image on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "changing-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Visualise the function of a kernel ====\n",
    "kernel1_L1\n",
    "\n",
    "def Convolution(image, kernel):\n",
    "    # Create the result matrix\n",
    "    res = np.zeros(image.shape[0], image.shape[1])\n",
    "\n",
    "    # Iteratively moving the kernel over the image\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            window = image[i:i+2, j:j+2]    # Specify a window (size equals to size of kernel), moving with the kernel\n",
    "            res[i, j] = (kernel * window).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sticky-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[-0.30638087 -0.1684348 ]\n",
      " [ 0.06103742  0.07483253]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23d3ece8648>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The second kernel in the first layer\n",
    "kernel2_L1 = layer1[:,:,0,1]\n",
    "print(kernel2_L1.shape)\n",
    "print(kernel2_L1)\n",
    "plt.imshow(kernel2_L1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
